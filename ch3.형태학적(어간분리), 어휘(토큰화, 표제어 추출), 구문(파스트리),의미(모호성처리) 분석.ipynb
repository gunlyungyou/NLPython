{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#morphological analysis(형태소에서 어간 생성하기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-8e7b81dff7fe>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-8e7b81dff7fe>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    https://www.lfd.uci.edu/~gohlke/pythonlibs/#pycld2\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#######polyglot 설치 오류 해결\n",
    "https://www.lfd.uci.edu/~gohlke/pythonlibs/#pycld2\n",
    "위 페이지 에서\n",
    "pip install PyICU-1.9.8-cp36-cp36m-win_amd64.whl\n",
    "pip install pycld2-0.31-cp36-cp36m-win_amd64.whl\n",
    "해당 whl파일들을 다운받은후 install\n",
    "\n",
    "git clone https://github.com/aboSamoor/polyglot.git\n",
    "cd polyglot\n",
    "python setup.py install\n",
    "\n",
    "pip install polyglot\n",
    "완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyglot_data] Downloading package morph2.en to\n",
      "[polyglot_data]     C:\\Users\\admin\\AppData\\Roaming\\polyglot_data...\n",
      "[polyglot_data]   Package morph2.en is already up-to-date!\n",
      "\n",
      "Derivational Morphemes\n",
      "happi\n",
      "unkind\n",
      "\n",
      "Inflectional  Morphemes\n",
      "dog\n",
      "expect\n",
      "\n",
      "Some examples\n",
      "unexpect\n",
      "disagr\n",
      "disagre\n",
      "agreement\n",
      "quirki\n",
      "gistor\n",
      "canon\n",
      "\n",
      "Deirvatinal Morphemes using polyglot library\n",
      "happiness           ['happi', 'ness']\n",
      "unkind              ['un', 'kind']\n",
      "\n",
      "Inflectional Morphemes using polyglot library\n",
      "dogs                ['dog', 's']\n",
      "expected            ['expect', 'ed']\n",
      "\n",
      "Some Morphemes examples using polyglot library\n",
      "dogs                ['dog', 's']\n",
      "expected            ['expect', 'ed']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from polyglot.text import Text, Word\n",
    "from polyglot.downloader import downloader\n",
    "downloader.download(\"morph2.en\")\n",
    "\n",
    "word = \"unexpected\"\n",
    "text = \"disagreement\"\n",
    "text1 = 'disagree'\n",
    "text2 = \"agreement\"\n",
    "text3 = \"quirkiness\"\n",
    "text4 = \"gistorical\"\n",
    "text5 = \"canonical\"\n",
    "text6 = \"happiness\"\n",
    "text7 = \"unkind\"\n",
    "text8 = \"dogs\"\n",
    "text9 = \"expected\"\n",
    "words_derv = [\"happiness\", \"unkind\"]\n",
    "word_infle = [\"dogs\", \"expected\"]\n",
    "words = [\"unexpected\", \"disagreement\", \"disagree\", \"agreement\", \"quirkness\", \"canonical\",\"historical\"]\n",
    "\n",
    "def stemmer_porter():\n",
    "    port = PorterStemmer()\n",
    "    print(\"\\nDerivational Morphemes\")\n",
    "    print(\" \".join([port.stem(i) for i in text6.split()]))\n",
    "    print(\" \".join([port.stem(i) for i in text7.split()]))\n",
    "    print(\"\\nInflectional  Morphemes\")\n",
    "    print(\" \".join([port.stem(i) for i in text8.split()]))\n",
    "    print(\" \".join([port.stem(i) for i in text9.split()])) \n",
    "    print(\"\\nSome examples\") \n",
    "    print(\" \".join([port.stem(i) for i in word.split()])) \n",
    "    print(\" \".join([port.stem(i) for i in text.split()])) \n",
    "    print(\" \".join([port.stem(i) for i in text1.split()])) \n",
    "    print(\" \".join([port.stem(i) for i in text2.split()]))\n",
    "    print(\" \".join([port.stem(i) for i in text3.split()])) \n",
    "    print(\" \".join([port.stem(i) for i in text4.split()])) \n",
    "    print(\" \".join([port.stem(i) for i in text5.split()])) \n",
    "    \n",
    "def polyglot_stem():\n",
    "    print(\"\\nDeirvatinal Morphemes using polyglot library\")\n",
    "    for w in words_derv: \n",
    "        w = Word(w, language=\"en\") \n",
    "        print(\"{:<20}{}\".format(w, w.morphemes)) \n",
    "\n",
    "    print(\"\\nInflectional Morphemes using polyglot library\")\n",
    "    for w in word_infle:\n",
    "        w = Word(w,language = \"en\")\n",
    "        print(\"{:<20}{}\".format(w, w.morphemes))\n",
    "        \n",
    "    print(\"\\nSome Morphemes examples using polyglot library\")\n",
    "    for w in word_infle:\n",
    "        w = Word(w, language = \"en\")\n",
    "        print(\"{:<20}{}\".format(w, w.morphemes))\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    stemmer_porter()\n",
    "    polyglot_stem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-b7cec687d1cc>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-b7cec687d1cc>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    1.다음 단어들의 접두사, 접미사, 동사, 어간을 정의하라\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#연습문제\n",
    "1.다음 단어들의 접두사, 접미사, 동사, 어간을 정의하라\n",
    "redness : 어간 - red, 접미사 - ness\n",
    "quickly : 어간 - quick, 접미사 - ly\n",
    "teacher : 어간 - teach, 접미사 - er\n",
    "unhappy : 접두사 - un, 어간 - happy\n",
    "disagreement : 접두사 - dis, 어간 - agree, 접미사 - ment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red', 'quickli', 'teacher', 'unhappi', 'disagr']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. 위 단어들의 어간을 만들어라\n",
    "from nltk.stem import PorterStemmer\n",
    "words = ['redness', 'quickly', 'teacher', 'unhappy', 'disagreement']\n",
    "port = PorterStemmer()\n",
    "\n",
    "morphs = []\n",
    "for j in range(len(words)):\n",
    "    morphs = morphs + [port.stem(i) for i in words[j].split()]\n",
    "\n",
    "morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyglot_data] Downloading package morph2.en to\n",
      "[polyglot_data]     C:\\Users\\admin\\AppData\\Roaming\\polyglot_data...\n",
      "[polyglot_data]   Package morph2.en is already up-to-date!\n",
      "['red', 'ness']\n",
      "['quick', 'ly']\n",
      "['teacher']\n",
      "['un', 'happy']\n",
      "['disagree', 'ment']\n"
     ]
    }
   ],
   "source": [
    "from polyglot.text import Text, Word\n",
    "from polyglot.downloader import downloader\n",
    "downloader.download(\"morph2.en\")\n",
    "\n",
    "for i in words:\n",
    "    i = Word(i, language = 'en')\n",
    "    print(i.morphemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.disagree, disagreement, historical을 어간과 어근으로 나누어라\n",
    "new_words = ['disagree', 'disagreement', 'historical']\n",
    "\n",
    "morphs = []\n",
    "for j in range(len(new_words)):\n",
    "    morphs = morphs + [port.stem(i) for i in new_words[j].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disagre', 'disagr', 'histor']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morphs #어간 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##tokenization & lemmatization(표제어 추출)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stemming', 'is', 'funnier', 'than', 'a', 'bummer', 'says', 'the', 'sushi', 'loving', 'computer', 'scientist', '.', 'She', 'really', 'wants', 'to', 'buy', 'cars', '.', 'She', 'told', 'me', 'angrily', '.', 'It', 'is', 'better', 'for', 'you', '.', 'Man', 'is', 'walking', '.', 'We', 'are', 'meeting', 'tomorrow', '.', 'You', 'really', 'do', \"n't\", 'know..', '!']\n",
      "\n",
      "\n",
      "----------Word Lemmatiation----------\n",
      "car\n",
      "walk\n",
      "meeting\n",
      "good\n",
      "be\n",
      "funny\n",
      "expect\n",
      "fantasize\n"
     ]
    }
   ],
   "source": [
    "def wordtokenization():\n",
    "    content = \"\"\"Stemming is funnier than a bummer says the sushi loving computer scientist. \n",
    "    She really wants to buy cars. She told me angrily. It is better for you. \n",
    "    Man is walking. We are meeting tomorrow. You really don't know..!\"\"\" \n",
    "    print(word_tokenize(content))\n",
    "\n",
    "\n",
    "def wordlemmatization():\n",
    "    wordlemma = WordNetLemmatizer()\n",
    "    print(wordlemma.lemmatize('cars'))\n",
    "    print(wordlemma.lemmatize('walking', pos = 'v'))\n",
    "    print(wordlemma.lemmatize('meeting', pos = 'n'))\n",
    "    print(wordlemma.lemmatize('better', pos = 'a'))\n",
    "    print(wordlemma.lemmatize('is', pos = 'v'))\n",
    "    print(wordlemma.lemmatize('funnier', pos = 'a'))\n",
    "    print(wordlemma.lemmatize('expected', pos = 'v'))\n",
    "    print(wordlemma.lemmatize('fantasized', pos = 'v'))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    wordtokenization()\n",
    "    print(\"\\n\")\n",
    "    print(\"----------Word Lemmatiation----------\")\n",
    "    wordlemmatization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Parsing result as per defines grammar----------\n",
      "(S (NP I) (VP (V shot) (NP (Det an) (N elephant))))\n",
      "\n",
      "----------Drawing Parse Tree----------\n",
      "(s (dp (d the) (np dog)) (vp (v chased) (dp (d the) (np cat))))\n",
      "\\Tree [.s\n",
      "        [.dp [.d the ] [.np dog ] ]\n",
      "        [.vp [.v chased ] [.dp [.d the ] [.np cat ] ] ] ]\n",
      "              s               \n",
      "      ________|_____           \n",
      "     |              vp        \n",
      "     |         _____|___       \n",
      "     dp       |         dp    \n",
      "  ___|___     |      ___|___   \n",
      " d       np   v     d       np\n",
      " |       |    |     |       |  \n",
      "the     dog chased the     cat\n",
      "\n",
      "\n",
      "----------Stanford Parser result----------\n",
      "(ROOT\n",
      "  (S\n",
      "    (NP (PRP I))\n",
      "    (VP (VBD shot)\n",
      "      (NP (DT an) (NN elephant)))\n",
      "    (. .)))\n",
      "(ROOT\n",
      "  (S\n",
      "    (NP (NNP School))\n",
      "    (VP (VB go)\n",
      "      (PP (TO to)\n",
      "        (NP (NN boy))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "###구문분석(parsing_tree)\n",
    "import nltk\n",
    "from nltk import CFG\n",
    "from nltk.tree import *\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from collections import defaultdict\n",
    "\n",
    "#1.nltk를 이용해 문법을 정의하고 parse의 결과를 생성한다.\n",
    "def definegrammar_parseresult():\n",
    "    Grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    PP -> P NP\n",
    "    NP -> Det N | Det N PP | 'I'\n",
    "    VP -> V NP | VP PP\n",
    "    Det -> 'an' | 'my'\n",
    "    N -> 'elephant' | 'pajamas'\n",
    "    V -> 'shot'\n",
    "    P -> 'in'\n",
    "    \"\"\")\n",
    "    sent = \"I shot an elephant\".split()\n",
    "    parser = nltk.ChartParser(Grammar)\n",
    "    trees = parser.parse(sent)\n",
    "    for tree in trees:\n",
    "        print(tree)\n",
    "        \n",
    "#2.parse tree를 그린다.\n",
    "def draw_parser_tree():\n",
    "    dp1 = Tree('dp', [Tree('d', ['the']), Tree('np', ['dog'])])\n",
    "    dp2 = Tree('dp', [Tree('d', ['the']), Tree('np', ['cat'])])\n",
    "    vp = Tree('vp', [Tree('v', ['chased']), dp2])\n",
    "    tree = Tree('s', [dp1, vp])\n",
    "    print(tree)\n",
    "    print(tree.pformat_latex_qtree())\n",
    "    tree.pretty_print()\n",
    "    \n",
    "def stanford_parsing_result():\n",
    "    text = \"\"\"I shot an elephant. The dog chased the cat. School go to boy.\"\"\"\n",
    "    nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "    res = nlp.annotate(text, properties = {\n",
    "        'annotators' : 'tokenize, ssplit, pos, depparse, parse',\n",
    "        'outputFormat' : 'json'\n",
    "    })\n",
    "    print(res['sentences'][0]['parse'])\n",
    "    print(res['sentences'][2]['parse'])\n",
    "\n",
    "#for coreNLP\n",
    "#cd stanford-corenlp-full-2018-02-27\n",
    "#java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n----------Parsing result as per defines grammar----------\")\n",
    "    definegrammar_parseresult()\n",
    "    print(\"\\n----------Drawing Parse Tree----------\")\n",
    "    draw_parser_tree()\n",
    "    print(\"\\n----------Stanford Parser result----------\")\n",
    "    stanford_parsing_result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
